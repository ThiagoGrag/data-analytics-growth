{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d08244e-15ef-487e-9546-edef5a201917",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Código para reduzir a base ao invés de aumentar a base minoritária.\n",
    "## Algumas funções não rodam no databrick community edition e para não deixar muito complexo vou simplesmente reduzir a base e depois rodar o modelo de clusterização\n",
    "## Para o teste AB vou rodar os testes de hipótese normalmente com o ticket médio e frequência\n",
    "## Para o modelo de clusterização vou rodar somente com 6 variáveis e com a base reduzida\n",
    "# Pacotes\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Carregando em PySpark\n",
    "# Seleção de colunas que vou utilizar no modelo. Já desempacotando utilizando o '*'\n",
    "selected_cols = ['delivery_region','origin_platform','delivery_time_class','price_range','minimum_order_value_class','order_total_amount','is_target','customer_id','order_id'] \n",
    "df = spark.table(\"silver.orders_time_distinct_class\").select(*selected_cols)\n",
    "\n",
    "\n",
    "# Separando as classes\n",
    "df_major = df.filter(F.col(\"is_target\") == 'target')  # classe majoritária \n",
    "df_minor = df.filter(F.col(\"is_target\") == 'control')  # classe minoritária\n",
    "\n",
    "# Contagens atuais\n",
    "count_major = df_major.count()\n",
    "count_minor = df_minor.count()\n",
    "\n",
    "# Calcule o fator de sobreamostragem para igualar a majoritária\n",
    "sampling_ratio = count_minor / count_major\n",
    "\n",
    "# Inserindo a classe minoritária com reposição\n",
    "df_major_downsampled = df_major.orderBy(F.rand(seed=42)).limit(count_minor)\n",
    "\n",
    "# Combinando o conjunto com a majoritária original\n",
    "df_balanced = df_minor.union(df_major_downsampled)\n",
    "\n",
    "# Verifique o balanceamento final\n",
    "df_balanced.groupBy(\"is_target\").count().show()\n",
    "df_balanced.display()\n",
    "\n",
    "# Salvando\n",
    "spark.sql(\"DROP TABLE IF EXISTS gold.orders_model\")\n",
    "df_balanced.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"gold.orders_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "144519d2-f4fa-451f-b1cc-ebb2049896a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Código para utilizar o modelo K-Prototype para clusterizar a base\n",
    "\n",
    "# Pacotes\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "import pandas as pd\n",
    "\n",
    "# Lendo base\n",
    "df = spark.table(\"gold.orders_model\")\n",
    "df_pd = df.toPandas()\n",
    "\n",
    "# Retirando variáveis que não vou utilizar no modelo\n",
    "var = df_pd[['customer_id','is_target','order_id']]\n",
    "df_pd = df_pd.drop(columns=['customer_id','is_target','order_id'])\n",
    "\n",
    "# Variáveis categóricas e categóricas numéricas\n",
    "cat_col = ['delivery_region','origin_platform']\n",
    "cat_col_num = ['delivery_time_class','price_range','minimum_order_value_class']\n",
    "num_col = ['order_total_amount']\n",
    "\n",
    "# Aplicação do labelEncoder\n",
    "le_dict = {}\n",
    "for col in cat_col:\n",
    "  le = LabelEncoder()\n",
    "  df_pd[col] = le.fit_transform(df_pd[col])\n",
    "  le_dict[col] = le\n",
    "\n",
    "# Padronizando a variável numérica\n",
    "scaler = StandardScaler()\n",
    "df_pd[num_col] = scaler.fit_transform(df_pd[num_col])\n",
    "\n",
    "# Unir todas as variáveis\n",
    "cat_colls = cat_col + cat_col_num\n",
    "cat_indices = [df_pd.columns.get_loc(col) for col in cat_colls]\n",
    "\n",
    "# Aplicação do modelo K-Prototype\n",
    "X = df_pd.values\n",
    "kproto = KPrototypes(n_clusters=3, init='Huang',n_init = 2 , verbose = 1, random_state=42) # Huang para variáveis numéricas e categóricas\n",
    "clusters = kproto.fit_predict(X, categorical=cat_indices)\n",
    "\n",
    "# Inserindo os clusters e voltando com os dados iniciais\n",
    "df_pd['cluster'] = clusters\n",
    "df_final = pd.concat([var.reset_index(drop=True), df_pd], axis = 1)\n",
    "\n",
    "# Salvando\n",
    "df_final_spark = spark.createDataFrame(df_final)\n",
    "df_final_spark.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"gold.orders_model_cluster_result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1109b3d7-26bb-4806-b1e5-3f95ce743e1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Código para voltar com os dados do tipo que spark permite\n",
    "import numpy as np\n",
    "\n",
    "def convert_types_for_spark(df):\n",
    "    for col, dtype in df.dtypes.items():\n",
    "        if np.issubdtype(dtype, np.integer):\n",
    "            # Converter inteiros menores para int32 ou int64\n",
    "            if dtype in [np.int8, np.int16]:\n",
    "                df[col] = df[col].astype(np.int32)\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.int64)\n",
    "        elif np.issubdtype(dtype, np.floating):\n",
    "            # Converter floats menores para float64\n",
    "            if dtype != np.float64:\n",
    "                df[col] = df[col].astype(np.float64)\n",
    "        elif dtype == 'object':\n",
    "            # Pode deixar como está (strings)\n",
    "            pass\n",
    "        else:\n",
    "            # Se tiver outros tipos, converter para string para evitar erros\n",
    "            df[col] = df[col].astype(str)\n",
    "    return df\n",
    "\n",
    "# Usar assim\n",
    "df_final = convert_types_for_spark(df_final)\n",
    "\n",
    "df_final_spark = spark.createDataFrame(df_final)\n",
    "df_final_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold.orders_segmentados\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afa4a4de-9c63-4176-bcfa-b832b7236f02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Código para voltar com os dados de valores originais\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Lendo base\n",
    "df = spark.table(\"gold.orders_segmentados\")\n",
    "df_order = spark.table(\"gold.orders_model\")\n",
    "\n",
    "\n",
    "# Seleção de colunas\n",
    "colunas_segmentados = ['customer_id','order_id','origin_platform','price_range','minimum_order_value_class','delivery_time_class','delivery_region','is_target','cluster']\n",
    "coluna_order = ['order_id','order_total_amount']\n",
    "\n",
    "# Novo df\n",
    "df_select = df.select(colunas_segmentados)\n",
    "df_order_select = df_order.select(coluna_order)\n",
    "\n",
    "# Gerando aliases\n",
    "order = df_select.alias('order')\n",
    "df_order = df_order_select.alias('df_order')\n",
    "\n",
    "\n",
    "# Join\n",
    "df_final = order \\\n",
    "    .join(df_order, col('order.order_id') == col('df_order.order_id'), 'left') \\\n",
    "    .select(\n",
    "        col('order.*'),\n",
    "        col('df_order.order_total_amount')\n",
    "        )\n",
    "    \n",
    "# Salvando  \n",
    "spark.sql(\"DROP TABLE IF EXISTS gold.orders_model_cluster_result\")  \n",
    "df_final.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"gold.orders_model_cluster_result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a620511-a8e3-4a1f-a390-88c12aaff1df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Código para gerar a base reduzido para a análise do teste AB\n",
    "# Pacotes\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Carregando em PySpark\n",
    "# Seleção de colunas que vou utilizar no modelo. Já desempacotando utilizando o '*'\n",
    "selected_cols = ['delivery_region','origin_platform','delivery_time_class','price_range','minimum_order_value_class','order_total_amount','is_target','customer_id','order_id','order_created_at'] \n",
    "df = spark.table(\"silver.orders_time_distinct_class\").select(*selected_cols)\n",
    "\n",
    "\n",
    "# Separando as classes\n",
    "df_major = df.filter(F.col(\"is_target\") == 'target')  # classe majoritária \n",
    "df_minor = df.filter(F.col(\"is_target\") == 'control')  # classe minoritária\n",
    "\n",
    "# Contagens atuais\n",
    "count_major = df_major.count()\n",
    "count_minor = df_minor.count()\n",
    "\n",
    "# Calcule o fator de sobreamostragem para igualar a majoritária\n",
    "sampling_ratio = count_minor / count_major\n",
    "\n",
    "# Inserindo a classe majoritária para a redução\n",
    "df_major_downsampled = df_major.orderBy(F.rand(seed=42)).limit(count_minor)\n",
    "\n",
    "# Combinando o conjunto com a minoritária original\n",
    "df_balanced = df_minor.union(df_major_downsampled)\n",
    "\n",
    "# Verifique o balanceamento final\n",
    "#df_balanced.groupBy(\"is_target\").count().show()\n",
    "df_balanced.display()\n",
    "\n",
    "# Salvando\n",
    "df_balanced.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"gold.orders_model_abtest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bd214b1-7cc7-4c96-89ea-86a3909864b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criar subsets para facilitar a utilização do ANOVA - Base com a variável platform\n",
    "\n",
    "# Lendo a base\n",
    "df = spark.table(\"gold.orders_model_abtest\")\n",
    "\n",
    "# Criação da view temporária\n",
    "df.createOrReplaceTempView(\"orders_model_abtest\")\n",
    "\n",
    "# SQL\n",
    "resultado_platform = spark.sql(\"\"\"\n",
    "  SELECT \n",
    "        customer_id,\n",
    "        ROUND(SUM(order_total_amount), 2)                      AS vlr_order,\n",
    "        ROUND(SUM(order_total_amount) / COUNT(order_id), 3)    AS ticket_medio,\n",
    "        ROUND(COUNT(order_id) / COUNT(DISTINCT customer_id), 3)  AS frequencia,\n",
    "        origin_platform,\n",
    "        is_target\n",
    "    FROM orders_model_abtest\n",
    "    GROUP BY customer_id, origin_platform, is_target\n",
    "\"\"\")\n",
    "\n",
    "resultado_price_range = spark.sql(\"\"\"\n",
    "  SELECT \n",
    "        customer_id,\n",
    "        ROUND(SUM(order_total_amount), 2)                      AS vlr_order,\n",
    "        ROUND(SUM(order_total_amount) / COUNT(order_id), 3)    AS ticket_medio,\n",
    "        ROUND(COUNT(order_id) / COUNT(DISTINCT customer_id), 3)  AS frequencia,\n",
    "        price_range,\n",
    "        is_target\n",
    "    FROM orders_model_abtest\n",
    "    GROUP BY customer_id, price_range, is_target\n",
    "\"\"\")\n",
    "\n",
    "resultado_minimum_value = spark.sql(\"\"\"\n",
    "  SELECT \n",
    "        customer_id,\n",
    "        ROUND(SUM(order_total_amount), 2)                      AS vlr_order,\n",
    "        ROUND(SUM(order_total_amount) / COUNT(order_id), 3)    AS ticket_medio,\n",
    "        ROUND(COUNT(order_id) / COUNT(DISTINCT customer_id), 3)  AS frequencia,\n",
    "        minimum_order_value_class,\n",
    "        is_target\n",
    "    FROM orders_model_abtest\n",
    "    GROUP BY customer_id, minimum_order_value_class, is_target\n",
    "\"\"\")\n",
    "\n",
    "resultado_delivery_time = spark.sql(\"\"\"\n",
    "  SELECT \n",
    "        customer_id,\n",
    "        ROUND(SUM(order_total_amount), 2)                      AS vlr_order,\n",
    "        ROUND(SUM(order_total_amount) / COUNT(order_id), 3)    AS ticket_medio,\n",
    "        ROUND(COUNT(order_id) / COUNT(DISTINCT customer_id), 3)  AS frequencia,\n",
    "        delivery_time_class,\n",
    "        is_target\n",
    "    FROM orders_model_abtest\n",
    "    GROUP BY customer_id, delivery_time_class, is_target\n",
    "\"\"\")\n",
    "\n",
    "resultado_region = spark.sql(\"\"\"\n",
    "  SELECT \n",
    "        customer_id,\n",
    "        ROUND(SUM(order_total_amount), 2)                      AS vlr_order,\n",
    "        ROUND(SUM(order_total_amount) / COUNT(order_id), 3)    AS ticket_medio,\n",
    "        ROUND(COUNT(order_id) / COUNT(DISTINCT customer_id), 3)  AS frequencia,\n",
    "        delivery_region,\n",
    "        is_target\n",
    "    FROM orders_model_abtest\n",
    "    GROUP BY customer_id, delivery_region, is_target\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS gold.abtest_platform\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS gold.abtest_price_range\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS gold.abtest_minimum_value\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS gold.abtest_delivery_time\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS gold.abtest_region\")\n",
    "\n",
    "# Salvando\n",
    "resultado_platform.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"gold.abtest_platform\")\n",
    "resultado_price_range.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"gold.abtest_price_range\")\n",
    "resultado_minimum_value.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"gold.abtest_minimum_value\")\n",
    "resultado_delivery_time.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"gold.abtest_delivery_time\")\n",
    "resultado_region.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"gold.abtest_region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43eee7db-4739-4f80-84a7-ea9f44ba5736",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criar subsets para facilitar a utilização do ANOVA - Base com a variável platform\n",
    "\n",
    "# Lendo a base\n",
    "df = spark.table(\"gold.orders_model_cluster_result\")\n",
    "\n",
    "# Criação da view temporária\n",
    "df.createOrReplaceTempView(\"orders_model_cluster_result\")\n",
    "\n",
    "# SQL\n",
    "resultado_cluster= spark.sql(\"\"\"\n",
    "  SELECT \n",
    "        customer_id,\n",
    "        ROUND(SUM(order_total_amount), 2)                      AS vlr_order,\n",
    "        ROUND(SUM(order_total_amount) / COUNT(order_id), 3)    AS ticket_medio,\n",
    "        ROUND(COUNT(order_id) / COUNT(DISTINCT customer_id), 3)  AS frequencia,\n",
    "        cluster,\n",
    "        is_target\n",
    "    FROM orders_model_cluster_result\n",
    "    GROUP BY customer_id, is_target, cluster\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS gold.abtest_cluster\")\n",
    "\n",
    "# Salvando\n",
    "resultado_cluster.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"gold.abtest_cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eeb844d5-cf47-4c50-be74-369c056f39d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Código para baixo seria para aumentar a base minoritária com SMOTE-NC ou com PySpark, mas decidi seguir com a redução tendo em vista que o método para \n",
    "# aumentar a base é simplesmente uma duplicação de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "754bd8d7-5005-47f9-97f5-ed43ef242adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Código para criar uma base na camada Gold já com informações balanceadas e com variáveis tratadas para a criação do Cluster de clientes\n",
    "## Para o balanceamento vou utilizar o SMOTE-NC para lidar com variáveis categóricas e numéricas\n",
    "## Aplica o labelEncoder para transformar as variáveis do tipo string em numéricas inteiras\n",
    "## Separo o que é variável categórica com numérica e aplico o modelo de balanceamento da variável 'target' com menor representação (aumentar a variável controle do is_target)\n",
    "### Depois aplico o modelo K-Prototype para criar a clusterização de clientes baseada em 6 variáveis\n",
    "### Serão 5 variáveis categóricas e 1 numérica. Vou criar 3 clusters\n",
    "#### Como estou utilizando o community edition não consigo instalar algumas bibliotecas\n",
    "#### Vou ter que utilizar funções nativas do PySpark para 'balancear' a base repitindo os valores da classe minoritária\n",
    "\n",
    "\n",
    "# ===============\n",
    "# CÓDIGO NÂO UTILIZADO\n",
    "# ===============\n",
    "# Pacotes\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Carregando em PySpark e depois transformando em Python devido a possibilidade de utilizar SMOTENC, mesmo em python sendo processado em RAM\n",
    "# Seleção de colunas que vou utilizar no modelo. Já desempacotando utilizando o '*'\n",
    "selected_cols = ['delivery_region','origin_platform','delivery_time_class','price_range','minimum_order_value_class','order_total_amount','is_target'] \n",
    "df_spark = spark.table(\"silver.orders_time_distinct_class\").select(*selected_cols)\n",
    "\n",
    "# Transformando em Pandas\n",
    "df = df_spark.toPandas()\n",
    "\n",
    "# Separação de variáveis categóricas e numéricas\n",
    "cat = ['delivery_region','origin_platform','delivery_time_class','price_range','minimum_order_value_class']\n",
    "num = ['order_total_amount']\n",
    "target = ['is_target']\n",
    "\n",
    "# Variáveis categóricas que precisam passar pelo LabelEncoder\n",
    "cat_string = ['delivery_region','origin_platform']\n",
    "\n",
    "# Aplicação do LabelEncoder\n",
    "le_dict = {}\n",
    "for col in cat_string:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    le_dict[col] = le\n",
    "\n",
    "X = df[cat + num]\n",
    "y = df[target]\n",
    "\n",
    "# Criando o índice\n",
    "cat_indices = [X.columns.get_loc(col) for col in cat]\n",
    "\n",
    "# Aplicação do SMOTE-NC\n",
    "# random_state = 42 para garantir a reprodutibilidade dos testes\n",
    "smote = SMOTENC(categorical_features=cat_indices, random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Juntando tudo novamente\n",
    "df_balanced = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled, name=target)], axis=1)\n",
    "\n",
    "\n",
    "# Salvando\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS gold\")\n",
    "#spark.sql(\"DROP TABLE IF EXISTS gold.modeled_orders\")\n",
    "df_balanced.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"gold.modeled_orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a54bf2a-2a4f-4206-94e7-0191b321ca78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============\n",
    "# CÓDIGO NÂO UTILIZADO\n",
    "# ===============\n",
    "# Salvando uma base do modelo sem alguns campos para verificação de balanceamento\n",
    "selected_cols = ['delivery_region','origin_platform','delivery_time_class','price_range','minimum_order_value_class','order_total_amount','is_target'] \n",
    "df_spark = spark.table(\"silver.orders_time_distinct_class\").select(*selected_cols)\n",
    "\n",
    "# Salvando\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS gold\")\n",
    "#spark.sql(\"DROP TABLE IF EXISTS gold.modeled_orders\")\n",
    "df_spark.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"gold.orders_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1f27554-f292-447d-b65f-b408d305590c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============\n",
    "# CÓDIGO NÂO UTILIZADO\n",
    "# ===============\n",
    "# Pacotes\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Carregando em PySpark\n",
    "# Seleção de colunas que vou utilizar no modelo. Já desempacotando utilizando o '*'\n",
    "selected_cols = ['delivery_region','origin_platform','delivery_time_class','price_range','minimum_order_value_class','order_total_amount','is_target'] \n",
    "df = spark.table(\"silver.orders_time_distinct_class\").select(*selected_cols)\n",
    "\n",
    "\n",
    "# Separando as classes\n",
    "df_major = df.filter(F.col(\"is_target\") == 'target')  # classe majoritária (ajuste o valor conforme)\n",
    "df_minor = df.filter(F.col(\"is_target\") == 'control')  # classe minoritária\n",
    "\n",
    "# Contagens atuais\n",
    "count_major = df_major.count()\n",
    "count_minor = df_minor.count()\n",
    "\n",
    "# Calcule o fator de sobreamostragem para igualar a majoritária\n",
    "sampling_ratio = count_major / count_minor\n",
    "\n",
    "# Inserindo a classe minoritária com reposição\n",
    "df_minor_upsampled = df_minor.sample(withReplacement=True, fraction=sampling_ratio, seed=42)\n",
    "\n",
    "# Combinando o conjunto com a majoritária original\n",
    "df_balanced = df_major.union(df_minor_upsampled)\n",
    "\n",
    "# Verifique o balanceamento final\n",
    "#df_balanced.groupBy(\"is_target\").count().show()\n",
    "#df_balanced.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "model_order",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
