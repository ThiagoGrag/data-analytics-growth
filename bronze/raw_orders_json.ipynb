{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbed8653-9508-4023-9c9b-c7b021463e48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Fontes e bibliotecas\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.types import *\n",
    "import urllib.request # Requisição para baixar arquivo na internet\n",
    "\n",
    "\n",
    "# Parâmetros\n",
    "# Tinha um erro no link do arquivo: estava como 'sour-ce' e na verdade é 'source'\n",
    "s3_path = \"s3a://data-architect-test-source/order.json.gz\"\n",
    "bronze_path = \"dbfs:/mnt/bronze/raw_order\"\n",
    "\n",
    "# Baixar arquivo\n",
    "#urllib.request.urlretrieve(url, local_path)\n",
    "\n",
    "# Leitura do JSON\n",
    "df = spark.read.json(s3_path, multiLine=True)\n",
    "\n",
    "# Verificando\n",
    "df.columns        \n",
    "df.schema.simpleString()  \n",
    "display(df)\n",
    "\n",
    "# Escrevendo no DBFS\n",
    "# Não vou salvar nada em camada Bronze\n",
    "# df.write.format(\"delta\").mode(\"overwrite\").save(bronze_path)\n",
    "\n",
    "# Registro como tabela para manipulação SQL\n",
    "# spark.sql(f\"\"\"\n",
    "#          CREATE TABLE IF NOT EXISTS raw_order\n",
    "#          USING DELTA\n",
    "#          LOCATION '{bronze_path}'\n",
    "#          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20e3838a-0003-4f96-b98d-e40f1317ac10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Código para estudar o campo item\n",
    "from pyspark.sql.functions import explode, col, from_json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Leitura do JSON bruto\n",
    "s3_path = \"s3a://data-architect-test-source/order.json.gz\"\n",
    "df = spark.read.json(s3_path, multiLine=True)\n",
    "\n",
    "# Define o schema do campo \"items\", incluindo garnishItems\n",
    "garnish_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"quantity\", DoubleType(), True),\n",
    "    StructField(\"unitPrice\", StructType([StructField(\"value\", StringType(), True)]), True),\n",
    "    StructField(\"totalValue\", StructType([StructField(\"value\", StringType(), True)]), True),\n",
    "    StructField(\"categoryName\", StringType(), True),\n",
    "    StructField(\"externalId\", StringType(), True)\n",
    "])\n",
    "\n",
    "item_schema = ArrayType(StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"quantity\", DoubleType(), True),\n",
    "    StructField(\"unitPrice\", StructType([StructField(\"value\", StringType(), True)]), True),\n",
    "    StructField(\"totalValue\", StructType([StructField(\"value\", StringType(), True)]), True),\n",
    "    StructField(\"garnishItems\", ArrayType(garnish_schema), True)\n",
    "]))\n",
    "\n",
    "# Converte o campo items de string para array de structs\n",
    "df_com_array = df.withColumn(\"items_parsed\", from_json(\"items\", item_schema))\n",
    "\n",
    "# Explode os items\n",
    "df_items_exploded = df_com_array.select(\n",
    "    \"order_id\",\n",
    "    explode(\"items_parsed\").alias(\"item\")\n",
    ")\n",
    "\n",
    "# Explode os garnishItems dentro de cada item\n",
    "df_garnish = df_items_exploded.select(\n",
    "    \"order_id\",\n",
    "    col(\"item.name\").alias(\"item_name\"),\n",
    "    col(\"item.quantity\").alias(\"item_quantity\"),\n",
    "    col(\"item.unitPrice.value\").alias(\"item_unit_value\"),\n",
    "    col(\"item.totalValue.value\").alias(\"item_total_value\"),\n",
    "    explode(col(\"item.garnishItems\")).alias(\"garnish\")\n",
    ")\n",
    "\n",
    "# Garnish\n",
    "df_garnish_final = df_garnish.select(\n",
    "    \"order_id\",\n",
    "    \"item_name\",\n",
    "    \"item_quantity\",\n",
    "    \"item_unit_value\",\n",
    "    \"item_total_value\",\n",
    "    col(\"garnish.name\").alias(\"garnish_name\"),\n",
    "    col(\"garnish.quantity\").alias(\"garnish_quantity\"),\n",
    "    col(\"garnish.unitPrice.value\").alias(\"garnish_unit_value\"),\n",
    "    col(\"garnish.totalValue.value\").alias(\"garnish_total_value\"),\n",
    "    col(\"garnish.categoryName\").alias(\"garnish_category\")\n",
    ")\n",
    "\n",
    "display(df_garnish_final)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "403997c5-ef5a-4497-b414-aa5e7fcb41d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Código para verificar em SQL\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Avaliar um caso em específico\n",
    "df = spark.read.option(\"multiLine\", True).json(\"s3a://data-architect-test-source/order.json.gz\")\n",
    "\n",
    "# Criação da view temporária\n",
    "df.createOrReplaceTempView(\"orders_raw\")\n",
    "\n",
    "# SQL\n",
    "resultado = spark.sql(\"\"\"\n",
    "  SELECT min(order_created_at),max(order_created_at)\n",
    "  FROM orders_raw\n",
    "  --where order_id = \n",
    "  -- 'a60c8379657ab06d6324d045f3881e348979f5a41c6d98220a12d4ad3cb2f8ed' -- Verificação de valores altos\n",
    "  --('0000ef1aa26353a390e8b661aa824da157ed12d3c1d48c9835b962559008a011',\n",
    "  --'0005cd36901075f2f64b4a2814c3354770fb2601f779f3a0ab7e2a72e876d73d') -- Verificação de duplicidades\n",
    "  --'653d87f714e6e40260de58014bfe284bb8528852b63cae033203f26ebd58f786' -- Teste para encontrar o customer_id\n",
    "  --'4ff64b33b272c1886df21b63272220af6a82d1667dba70dad201810d98608dd8' \n",
    "  \n",
    "\"\"\")\n",
    "\n",
    "display(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7073b02-2c3b-481c-94d3-5374cf9fd3e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verificação de dados únicos\n",
    "from pyspark.sql.functions import countDistinct, count\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "s3_path = \"s3a://data-architect-test-source/order.json.gz\"\n",
    "bronze_path = \"dbfs:/mnt/bronze/raw_order\"\n",
    "\n",
    "# Leitura do JSON\n",
    "df = spark.read.json(s3_path, multiLine=True)\n",
    "\n",
    "# customer_id\n",
    "total_customer_id = df.select(\"customer_id\").count()\n",
    "distinct_customer_id = df.select(\"customer_id\").agg(countDistinct(\"customer_id\").alias(\"distinct\")).collect()[0][\"distinct\"]\n",
    "\n",
    "# order_id\n",
    "total_order_id = df.select(\"order_id\").count()\n",
    "distinct_order_id = df.select(\"order_id\").agg(countDistinct(\"order_id\").alias(\"distinct\")).collect()[0][\"distinct\"]\n",
    "\n",
    "# Resultado\n",
    "print(f\"Total de customer_id: {total_customer_id}\")\n",
    "print(f\"Customer_id distintos: {distinct_customer_id}\")\n",
    "print(f\"Total de order_id: {total_order_id}\")\n",
    "print(f\"Order_id distintos: {distinct_order_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4864ef39-42eb-4a54-897e-806100768937",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "raw_orders_json",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
