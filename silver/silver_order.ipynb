{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c02fd11b-343e-402e-a4c2-13f0717f11c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Código para selecionar colunas que vou dar sequência na análise\n",
    "# Fontes e bibliotecas\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Parâmetros\n",
    "s3_path = \"s3a://data-architect-test-source/order.json.gz\"\n",
    "bronze_path = \"dbfs:/mnt/bronze/raw_order\"\n",
    "s3_path_consumer = \"s3a://data-architect-test-source/consumer.csv.gz\"\n",
    "\n",
    "# Leitura do JSON\n",
    "df = spark.read.json(s3_path, multiLine=True)\n",
    "df_consumer = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(s3_path_consumer)\n",
    "df_teste_ab = spark.table('bronze.ab_test_ref')\n",
    "\n",
    "# Seleção de colunas\n",
    "colunas_selecionadas = ['customer_id', 'delivery_address_city', 'delivery_address_state' ,'order_id', 'merchant_id','order_scheduled','order_total_amount','origin_platform']\n",
    "\n",
    "# Novo df\n",
    "df_order_select = df.select(colunas_selecionadas)\n",
    "\n",
    "# Gerando aliases\n",
    "order = df_order_select.alias('order')\n",
    "consumer = df_consumer.alias('consumer')\n",
    "teste_ab = df_teste_ab.alias('test_ab')\n",
    "\n",
    "\n",
    "# Join com a tabela consumer para trazer o status e com o teste ab para trazer o grupo.\n",
    "df_final = order \\\n",
    "    .join(consumer, col('order.customer_id') == col('consumer.customer_id'), 'left') \\\n",
    "    .join(teste_ab, col('order.customer_id') == col('test_ab.customer_id'), 'left') \\\n",
    "    .select(\n",
    "        col('order.*'),\n",
    "        col('consumer.active'),\n",
    "        col('test_ab.is_target')\n",
    "        )\n",
    "    \n",
    "# Visualizando\n",
    "#df_final.display()\n",
    "\n",
    "# Salvando\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS silver\")\n",
    "df_final.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"silver.orders_enriched\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acb83eba-6d10-47b9-a632-dcc9150eb5df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Código para verificar campos nulos\n",
    "# Fontes e bibliotecas\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Parâmetros\n",
    "s3_path = \"s3a://data-architect-test-source/order.json.gz\"\n",
    "bronze_path = \"dbfs:/mnt/bronze/raw_order\"\n",
    "s3_path_consumer = \"s3a://data-architect-test-source/consumer.csv.gz\"\n",
    "\n",
    "# Leitura do JSON\n",
    "df = spark.read.json(s3_path, multiLine=True)\n",
    "df_consumer = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(s3_path_consumer)\n",
    "df_teste_ab = spark.table('bronze.ab_test_ref')\n",
    "\n",
    "# Seleção de colunas\n",
    "colunas_selecionadas = ['customer_id', 'delivery_address_city', 'delivery_address_state' ,'order_id', 'merchant_id','order_scheduled','order_total_amount','origin_platform']\n",
    "\n",
    "# Novo df\n",
    "df_order_select = df.select(colunas_selecionadas)\n",
    "\n",
    "# Gerando aliases\n",
    "order = df_order_select.alias('order')\n",
    "consumer = df_consumer.alias('consumer')\n",
    "teste_ab = df_teste_ab.alias('test_ab')\n",
    "\n",
    "\n",
    "# Join com a tabela consumer para trazer o status e com o teste ab para trazer o grupo.\n",
    "df_final = order \\\n",
    "    .join(consumer, col('order.customer_id') == col('consumer.customer_id'), 'left') \\\n",
    "    .join(teste_ab, col('order.customer_id') == col('test_ab.customer_id'), 'left') \\\n",
    "    .select(\n",
    "        col('order.*'),\n",
    "        col('consumer.active'),\n",
    "        col('test_ab.is_target')\n",
    "        )\n",
    "    \n",
    "\n",
    "from pyspark.sql.functions import countDistinct, count\n",
    "\n",
    "# Contar apenas customer_id não nulos\n",
    "non_null_customer_id = df_final.select(count(\"customer_id\").alias(\"non_null_customer_id\")).collect()[0][\"non_null_customer_id\"]\n",
    "distinct_customer_id = df_final.select(countDistinct(\"customer_id\").alias(\"distinct_customer_id\")).collect()[0][\"distinct_customer_id\"]\n",
    "\n",
    "# Contar apenas order_id não nulos\n",
    "non_null_order_id = df_final.select(count(\"order_id\").alias(\"non_null_order_id\")).collect()[0][\"non_null_order_id\"]\n",
    "distinct_order_id = df_final.select(countDistinct(\"order_id\").alias(\"distinct_order_id\")).collect()[0][\"distinct_order_id\"]\n",
    "\n",
    "# Resultado - Temos 8.505 dados sem informação do customer_id, então não temos a informação da atividade e se participou do teste ab. Vou excluir\n",
    "print(f\"customer_id (não nulos): {non_null_customer_id}\")\n",
    "print(f\"customer_id distintos (não nulos): {distinct_customer_id}\")\n",
    "print(f\"order_id (não nulos): {non_null_order_id}\")\n",
    "print(f\"order_id distintos (não nulos): {distinct_order_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "868543c7-acec-43ee-b9eb-31c47893d743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Código para estudar na linha do tempo algumas variáveis como a quantidade de pedidos, valores, cidade, estado e tipo de plataforma\n",
    "# Fontes e bibliotecas\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, to_date, when, count, isnan, lit, to_timestamp\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Parâmetros\n",
    "s3_path = \"s3a://data-architect-test-source/order.json.gz\"\n",
    "bronze_path = \"dbfs:/mnt/bronze/raw_order\"\n",
    "s3_path_consumer = \"s3a://data-architect-test-source/consumer.csv.gz\"\n",
    "s3_path_restaurant = \"s3a://data-architect-test-source/restaurant.csv.gz\"\n",
    "\n",
    "# Leitura do JSON\n",
    "df = spark.read.json(s3_path, multiLine=True)\n",
    "df_consumer = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(s3_path_consumer)\n",
    "df_restaurant = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(s3_path_restaurant)\n",
    "df_teste_ab = spark.table('bronze.ab_test_ref')\n",
    "\n",
    "# Seleção de colunas\n",
    "colunas_selecionadas = ['customer_id', 'delivery_address_city', 'delivery_address_state' ,'order_id', 'merchant_id','order_created_at','order_total_amount','origin_platform']\n",
    "\n",
    "# Novo df\n",
    "df_order_select = df.select(colunas_selecionadas)\n",
    "\n",
    "# Gerando aliases\n",
    "order = df_order_select.alias('order')\n",
    "consumer = df_consumer.alias('consumer')\n",
    "restaurant = df_restaurant.alias('restaurant')\n",
    "teste_ab = df_teste_ab.alias('test_ab')\n",
    "\n",
    "\n",
    "# Join com a tabela consumer para trazer o status e com o teste ab para trazer o grupo.\n",
    "df_final = order \\\n",
    "    .join(consumer, col('order.customer_id') == col('consumer.customer_id'), 'left') \\\n",
    "    .join(teste_ab, col('order.customer_id') == col('test_ab.customer_id'), 'left') \\\n",
    "    .join(restaurant, col('order.merchant_id') == col('restaurant.id'), 'left') \\\n",
    "    .select(\n",
    "        col('order.*'),\n",
    "        col('consumer.active'),\n",
    "        col('test_ab.is_target'),\n",
    "        col('restaurant.price_range'),\n",
    "        col('restaurant.delivery_time'),\n",
    "        col('restaurant.minimum_order_value')\n",
    "        )\n",
    "    \n",
    "df_analise = df_final.withColumn(\"order_created_at\", to_date(to_timestamp(col(\"order_created_at\"))))\n",
    "\n",
    "# Verificando mais nulidades\n",
    "# encontrei mais 2 no campo origin_platform e 1.271 no campo active -> Vou seguir com a exclusão para limpar a base\n",
    "#null_counts = df_final.select([\n",
    "#    count(when(col(c).isNull(), c)).alias(c)\n",
    "#    for c in df_final.columns\n",
    "#])\n",
    "#null_counts.show()\n",
    "\n",
    "# Remoção dos nulos do campo customer_id, active e origin_platform\n",
    "df_clean = df_analise.filter(\n",
    "    df_analise[\"customer_id\"].isNotNull() & df_analise['active'].isNotNull() & df_analise['origin_platform'].isNotNull() & df_analise['delivery_time'].isNotNull() & df_analise['minimum_order_value'].isNotNull()\n",
    ")\n",
    "    \n",
    "# Visualizando\n",
    "#df_final.display()\n",
    "\n",
    "# Salvando\n",
    "#spark.sql(\"CREATE DATABASE IF NOT EXISTS silver\")\n",
    "df_clean.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"silver.orders_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "685b6e18-51eb-4dc8-866b-71edff41a74e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Código para gerar a base sem as duplicações\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import concat_ws, count\n",
    "\n",
    "# Carregando a base\n",
    "df = spark.table('silver.orders_time')\n",
    "\n",
    "\n",
    "chaves = ['order_id','customer_id','order_total_amount','origin_platform']\n",
    "\n",
    "# Criar o concat\n",
    "df_chave = df.withColumn(\"chave_unica\", concat_ws(\"|\",*chaves))\n",
    "\n",
    "# Remove as duplicações e também a coluna auxiliar                   \n",
    "df_distinct = df_chave.dropDuplicates(['chave_unica']).drop('chave_unica')\n",
    "\n",
    "\n",
    "# Salvando\n",
    "df_distinct.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"silver.orders_time_distinct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91ffc2f1-1b84-455a-92b2-60be095a8dd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Código para ajustar as variáveis delivery_time e minimum_order_value, transformando elas categorias\n",
    "# regra delivery_time: < 19 sendo 1, >= 19 and < 45 sendo 2 e >= 45 sendo 3.\n",
    "# regra minimum_order_value: < 15 sendo 1, >= 15 and < 21 sendo 2 e >= 21 sendo 3.\n",
    "# Também vou excluir os dados com valores dos pedidos zerados e inserir a visão de região\n",
    "\n",
    "# Código para gerar a base\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Carregando a base\n",
    "df = spark.table('silver.orders_time_distinct')\n",
    "\n",
    "# Manipulação das colunas\n",
    "df_class = df \\\n",
    "    .withColumn(\n",
    "    'delivery_time_class',\n",
    "    when(col('delivery_time') < 19,1)\n",
    "    .when((col('delivery_time') >= 19) & (col('delivery_time') < 45),2)\n",
    "    .otherwise(3)\n",
    ") \\\n",
    "    .withColumn(\n",
    "    'minimum_order_value_class',\n",
    "    when(col('minimum_order_value') < 15,1)\n",
    "    .when((col('minimum_order_value') >= 15) & (col('minimum_order_value') < 21),2)\n",
    "    .otherwise(3)\n",
    ") \\\n",
    "    .withColumn(\n",
    "    'delivery_region',\n",
    "    when(col(\"delivery_address_state\").isin(\"AC\", \"AP\", \"AM\", \"PA\", \"RO\", \"RR\", \"TO\"), \"Norte\")\n",
    "    .when(col(\"delivery_address_state\").isin(\"AL\", \"BA\", \"CE\", \"MA\", \"PB\", \"PE\", \"PI\", \"RN\", \"SE\"), \"Nordeste\")\n",
    "    .when(col(\"delivery_address_state\").isin(\"DF\", \"GO\", \"MT\", \"MS\"), \"Centro-Oeste\")\n",
    "    .when(col(\"delivery_address_state\").isin(\"ES\", \"MG\", \"RJ\", \"SP\"), \"Sudeste\")\n",
    "    .when(col(\"delivery_address_state\").isin(\"PR\", \"RS\", \"SC\"), \"Sul\")\n",
    "    .otherwise(\"Desconhecido\")\n",
    "    )\n",
    "\n",
    "# Exclusão dos valores zerados do campo do valor do pedido\n",
    "df_clean = df_class.filter(col('order_total_amount') != 0)\n",
    "\n",
    "# Salvando\n",
    "spark.sql(\"DROP TABLE IF EXISTS silver.orders_time_distinct_class\")\n",
    "df_clean.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"silver.orders_time_distinct_class\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_order",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
